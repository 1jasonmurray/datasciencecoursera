---
title: "HAR - Correct Dumbbell Curl"
author: "Jason Murray"
date: '2017-02-27'
output: html_document
---

##Summary

The purpose of this analysis is to find out whether we can identify not only the activity being performed from sensor data but whether it is being done correctly.  If not being done properly we would also like to identify in what way so proper feedback can be given to improve performace.  For this analysis we will use the weight lifting excersise dataset generously provided here: 
http://groupware.les.inf.puc-rio.br/har   

The dataset includes sensor data collected from the arm, forearm, belt, and dumbbell.  The particpant was asked to perform unilateral dumbbell bicep curl correctly as well as 4 different examples of incorrect movements while supervised by a trainer, for a total of 5 classes.  

A - exactly according to specification   
B - throwing the elbows to the front    
C - lifting the dumbbell halfway   
D - lowering the dumbbell halfway    
E - Throwing the hips to the front   

We will try out a few different models and evaluate which one provides the best fitting and then consider whether it is acccurate enough to be built into a feeedback system for users.   

##Data Import and Cleaning

####Required Libraries
```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(caret)
library(parallel)
library(doParallel)
library(gbm)
library(randomForest)
library(kernlab)
```
   

The data for this analysis was obtained from the following locations:   
Training Data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv   
Testing Data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


```{r}
##Data download and import.
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

data_dir <- "./data"
train_file <- paste(data_dir, "/pml-training.csv", sep = "")
test_file <- paste(data_dir, "/pml-testing.csv", sep = "")

#download.file(train_url,train_file)
#download.file(test_url,test_file)

training <- read.csv(train_file, stringsAsFactors = FALSE)
testing <- read.csv(test_file, stringsAsFactors = FALSE)

str(training)

```

Right away we can see that are a lot of missing and NA values in our data.  Let's see how many lines are complete to see if we can figure out why this is.  

```{r}
sum(complete.cases(training))
```

Taking a quick look at the data it appears there are summary lines for each exercise window.  So the columns that are mostly imcomplete appear to be filled for these entries only.  There is a column labeled new_window, when set to yes it includes the summary data.  Let's double check this assumption by counting these lines.    

```{r}
nw_yes <- training %>% filter(new_window=='yes')
dim(nw_yes)
```

So this corresponds exactly to our complete cases.  Looking at the testing data there are no summary lines.  Based on this I'm not sure if the summary values are going to be of value in building a model to make predictions.  Let's clean up our data by keeping only the columns that have values for all entries.  Let's also exclude anything that isn't likely to help us predict, including the index number, user_name, timestamps, and the window number.  The selection below pulls all the columns that are complete and relevant to our analysis.     

```{r}
training <- training %>% select(matches("^(roll|pitch|yaw|total_accel|gyros|accel|magnet|classe)"))
dim(training)
sum(complete.cases(training))
```

So all the lines are now complete and we have reduced the variables from 160 to 53.   

Let's change the classe variable to a factor since we are classifying and declare our data clean and ready for analysis.

```{r}
training$classe <- as.factor(training$classe)
```

In this analysis we are looking to make final predictions on the test set once.  I would like to try out a few different models to pick the best one before trying to make this prediction.  In order to do that I'm going to split 30% of the training data out to use for validation testing of my models.

```{r}
inTrain <- createDataPartition(training$classe, p = 0.7, list = F)
train <- training[inTrain,]
validation <- training[-inTrain,]
```




##Model Creation

Now let's build a few models to try out on our validation data but first let's turn on parallel processing to speed things up.  
```{r, message=FALSE, warning=FALSE}
## Set up parallel processing
cluster <- makeCluster(detectCores() - 1) 
registerDoParallel(cluster)

```

For all of the models I will be using cross validation with 10 folds and 3 repeats.  We could potentially add few more repeats but I think this will add too much computation time and 3 repeats should be sufficient.  For each of the models I will also be searching for the best hyperparameters to use based on the dataset.  I have choosen random forest, gradient boosting, and svm for my models to test.  I will also combine them to see if I get better performance by stacking and will select the best of the 4 to generate my final test predictions.  I will rebuild the selected model(s) with all of the training data once I find the best one to maximize it's performance on the test data.   


####Random Forest
```{r, cache=TRUE, message=FALSE, warning=FALSE}
set.seed(42)
rf_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, allowParallel=TRUE)
tunegrid_rf <- expand.grid(.mtry=c(1:20))
model_rf <- train(classe ~ ., data=train, method='rf', preProcess = c("center", "scale"), trControl = rf_ctrl, tuneGrid = tunegrid_rf)
```

On the first run I didn't set the tuneGrid and it selected random values for mtry across a wide range.  The model selected 2 as the best fit but I wanted to see if there was something better near 2 so I ran again with 1-20 as possible values.  Here is a graph of their accuracy as well as the model info.  

```{r}
plot(model_rf)
model_rf
```

8 was selected as the best mtry parameter.  I will use a small range around 8 when building the final model.  

```{r}
model_rf$finalModel
```

The performance of this model seems pretty good, we will see how it does on the validation set.   

#### Stochastic Gradient Boosting
```{r, cache=TRUE, message=FALSE, warning=FALSE}
gbm_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, allowParallel=TRUE)
gbm_grid <- expand.grid(interaction.depth = (1:8), shrinkage = .1, n.trees = (0:10)*50, n.minobsinnode = 10)
model_gbm <- train(classe ~ ., data=train, method='gbm', preProcess = c("center", "scale"), trControl = gbm_ctrl, tuneGrid = gbm_grid)
```
I noticed during the first run of the gbm model with default values that both the interaction depth and the number of iterations had a positive influence on accuracy and were still growing.  So I made a second run with the values above to try and find better parameters. 

```{r}
plot(model_gbm)
model_gbm
```


```{r}
model_gbm$finalModel
```


####Radial Support Vector Machine
```{r, cache=TRUE, message=FALSE, warning=FALSE}
svm_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, allowParallel=TRUE)
model_svm <- train(classe ~ ., data=train, method='svmRadial', preProcess = c("center", "scale"), trControl = svm_ctrl, tuneLength = 15)
```

On the first model build of the SVM I noticed that Accuracy kept increasing with larger values of C.  I extended the tuneLength on the second run to 15 to see if I could find a peak.  

```{r}
plot(model_svm)
model_svm
```

As you can see the value selected was 512 which is what I will use in building the final model.

```{r}
model_svm$finalModel
```

The training error is farily low we will see how it performs on the validation data.  


##Model Testing

Now let's test our models against the validation data to see how they perform.  

```{r}
predict_rf <- predict(model_rf,newdata = validation)
predict_gbm <- predict(model_gbm,newdata = validation)
predict_svm <- predict(model_svm,newdata = validation)

confusionMatrix(predict_rf, validation$classe)
confusionMatrix(predict_gbm, validation$classe)
confusionMatrix(predict_svm, validation$classe)
```


####Stacked model
Let's put together a model stacking all 3 of these and see how it performs in comparison to the individual models.  

```{r}
stacked <- data.frame(predict_rf, predict_gbm, predict_svm, classe = train$classe)

stacked_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, allowParallel=TRUE)
model_stacked <- train(classe ~ ., data=stacked, method='rf', trControl = stacked_ctrl)
predict_stacked <- predict(model_stacked, newdata=validation)
```

##Summary


